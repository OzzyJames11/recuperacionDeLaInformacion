{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13767250,"sourceType":"datasetVersion","datasetId":8761785}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"6abdb1e6b987657b","cell_type":"markdown","source":"# Ejercicio 3: Preprocesamiento\n\n## Objetivo de la práctica\n\n1. Comprender y aplicar normalización, tokenización, stopwords, stemming y n-gramas.\n2. Medir el impacto de cada paso en el vocabulario y los tokens.","metadata":{}},{"id":"f3dbf9d17ef0f4ca","cell_type":"markdown","source":"#### 0. Cargar el Corpus\n\nVamos a trabajar con el corpus de Movie Reviews de IMDB","metadata":{}},{"id":"64c7cf74851c068c","cell_type":"code","source":"import pandas as pd","metadata":{"ExecuteTime":{"end_time":"2025-11-05T16:36:49.438778Z","start_time":"2025-11-05T16:36:45.845037Z"},"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T22:22:56.252077Z","iopub.execute_input":"2025-11-17T22:22:56.252388Z","iopub.status.idle":"2025-11-17T22:22:56.634060Z","shell.execute_reply.started":"2025-11-17T22:22:56.252366Z","shell.execute_reply":"2025-11-17T22:22:56.632935Z"}},"outputs":[],"execution_count":3},{"id":"initial_id","cell_type":"code","source":"path = '/kaggle/input/imdbdatasetzip/IMDB Dataset.csv'\ndf = pd.read_csv(path, encoding='utf-8')\nprint(df.head())\nprint(len(df))","metadata":{"ExecuteTime":{"end_time":"2025-11-05T16:45:10.801251Z","start_time":"2025-11-05T16:45:09.853018Z"},"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T22:22:56.635834Z","iopub.execute_input":"2025-11-17T22:22:56.636380Z","iopub.status.idle":"2025-11-17T22:22:58.164769Z","shell.execute_reply.started":"2025-11-17T22:22:56.636345Z","shell.execute_reply":"2025-11-17T22:22:58.163756Z"}},"outputs":[{"name":"stdout","text":"                                              review sentiment\n0  One of the other reviewers has mentioned that ...  positive\n1  A wonderful little production. <br /><br />The...  positive\n2  I thought this was a wonderful way to spend ti...  positive\n3  Basically there's a family where a little boy ...  negative\n4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n50000\n","output_type":"stream"}],"execution_count":4},{"id":"fdead2f940c14afd","cell_type":"markdown","source":"#### 1. Limpieza \n\nLimpiar los documentos de caracteres que no corresponden","metadata":{}},{"id":"12f65ae9-4646-4fbb-9efd-5469f89556a6","cell_type":"code","source":"# Limpieza extendida (recomendado)\nimport re\n\n# compilar patrones una sola vez\nTAG_RE    = re.compile(r'<.*?>')          # quita tags HTML simples\nCTRL_RE   = re.compile(r'[\\x00-\\x08\\x0b\\x0c\\x0e-\\x1f\\x7f]')  # chars de control\nPUNCT_RE  = re.compile(r'[^A-Za-z0-9\\s]') # todo lo que NO sea letra / número / espacio -> quitar\nMULTI_WS  = re.compile(r'\\s+')\n\ndef clean_text_extended(text):\n    \"\"\"Quita HTML, caracteres de control, signos/puntuación, y colapsa espacios.\n       Devuelve string limpio; si text no es str, devuelve empty string.\n    \"\"\"\n    if not isinstance(text, str):\n        return \"\"\n    t = text\n    # 1) quitar tags HTML (reemplazo por espacio)\n    t = TAG_RE.sub(\" \", t)\n    # 2) quitar caracteres de control (incluye \\x08)\n    t = CTRL_RE.sub(\"\", t)\n    # 3) quitar puntuación y símbolos no alfanuméricos (quita \\, (, ), ., ,, ', \", ?, !, -, etc)\n    t = PUNCT_RE.sub(\" \", t)\n    # 4) colapsar espacios múltiples y trim\n    t = MULTI_WS.sub(\" \", t).strip()\n    return t\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T22:22:58.166300Z","iopub.execute_input":"2025-11-17T22:22:58.166716Z","iopub.status.idle":"2025-11-17T22:22:58.174336Z","shell.execute_reply.started":"2025-11-17T22:22:58.166675Z","shell.execute_reply":"2025-11-17T22:22:58.173338Z"}},"outputs":[],"execution_count":5},{"id":"e89ca032-a728-46bf-819b-15fd0740c0d8","cell_type":"code","source":"# guardar el review original\ndf['review_original'] = df['review']\n\n# aplicar la limpieza extendida\ndf['review_clean'] = df['review'].apply(clean_text_extended)\n\n# comprobar cuántas filas cambiaron\nchanged_mask = df['review'] != df['review_clean']\nprint(\"Filas totales:\", len(df))\nprint(\"Filas cambiadas:\", changed_mask.sum())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T22:22:58.177033Z","iopub.execute_input":"2025-11-17T22:22:58.177391Z","iopub.status.idle":"2025-11-17T22:23:04.769084Z","shell.execute_reply.started":"2025-11-17T22:22:58.177368Z","shell.execute_reply":"2025-11-17T22:23:04.767960Z"}},"outputs":[{"name":"stdout","text":"Filas totales: 50000\nFilas cambiadas: 49996\n","output_type":"stream"}],"execution_count":6},{"id":"2acd9c33-f556-4b6e-b00a-fbedd8b43add","cell_type":"code","source":"# inspeccion de un elemento en especifico que tenia caracteres de control\n# se usa repr para visualizar los caracteres de control.\nidx = 14497\nprint(\"\\n=== ORIGINAL ===\")\nprint(repr(df['review_original'].iloc[idx][:300])) #se muestran los primeros 300 caracteres con [:300] en un string\nprint(\"\\n=== LIMPIO ===\")\nprint(repr(df['review_clean'].iloc[idx][:300]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T22:23:04.770271Z","iopub.execute_input":"2025-11-17T22:23:04.770610Z","iopub.status.idle":"2025-11-17T22:23:04.777364Z","shell.execute_reply.started":"2025-11-17T22:23:04.770581Z","shell.execute_reply":"2025-11-17T22:23:04.776114Z"}},"outputs":[{"name":"stdout","text":"\n=== ORIGINAL ===\n\"\\x08\\x08\\x08\\x08A Turkish Bath sequence in a film noir located in New York in the 50's, that must be a hint at something ! Something that curiously, in all the previous comments, no one has pointed out , but seems to me essential to the understanding of this movie <br /><br />the Turkish Baths sequence: a back \"\n\n=== LIMPIO ===\n'A Turkish Bath sequence in a film noir located in New York in the 50 s that must be a hint at something Something that curiously in all the previous comments no one has pointed out but seems to me essential to the understanding of this movie the Turkish Baths sequence a back street at night the entr'\n","output_type":"stream"}],"execution_count":7},{"id":"7da98f7ea5cae78d","cell_type":"markdown","source":"#### 2. Normalización\n\nConvertir todos los tokens a minúsculas.\n\nElimina puntuación y símbolos no alfabéticos.","metadata":{}},{"id":"3b86bae5f3eb43d7","cell_type":"code","source":"# Convertir a minúsculas + tokenizar\n# Ya se eliminó la puntuación en el paso anterior al quedarnos solo con lo que queremos (letras y números)\n# Aquí se eliminan los números puesto que son símbolos no alfabéticos\n\ndef normalize_text(text):\n    if not isinstance(text, str):\n        return []\n    # minúsculas\n    text = text.lower()\n    # tokenizar por espacios\n    tokens = text.split()\n    # eliminar tokens no alfabéticos (números)\n    tokens = [t for t in tokens if re.fullmatch(r'[a-z]+', t)]\n    return tokens\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T22:24:58.954467Z","iopub.execute_input":"2025-11-17T22:24:58.954866Z","iopub.status.idle":"2025-11-17T22:24:58.962268Z","shell.execute_reply.started":"2025-11-17T22:24:58.954841Z","shell.execute_reply":"2025-11-17T22:24:58.960710Z"}},"outputs":[],"execution_count":10},{"id":"7eb85d8d-0153-40e8-9b94-92b9bf1d054d","cell_type":"code","source":"# se crea la columna tokens_norm en el dataframe\ndf['tokens_norm'] = df['review_clean'].apply(normalize_text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T22:25:02.534431Z","iopub.execute_input":"2025-11-17T22:25:02.534787Z","iopub.status.idle":"2025-11-17T22:25:12.822106Z","shell.execute_reply.started":"2025-11-17T22:25:02.534759Z","shell.execute_reply":"2025-11-17T22:25:12.820857Z"}},"outputs":[],"execution_count":11},{"id":"b3358a6f-4483-4cb3-b799-95739883786a","cell_type":"code","source":"# Comparativa de la evolución del texto a tokens\nprint(\"\\n=== Texto Original ===\")\nprint(repr(df['review'].iloc[14497][:100]))\nprint(\"\\n=== Tokens Resultantes ===\")\nprint(df['tokens_norm'].iloc[14497][:15])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T22:25:15.971101Z","iopub.execute_input":"2025-11-17T22:25:15.971426Z","iopub.status.idle":"2025-11-17T22:25:15.978521Z","shell.execute_reply.started":"2025-11-17T22:25:15.971400Z","shell.execute_reply":"2025-11-17T22:25:15.977502Z"}},"outputs":[{"name":"stdout","text":"\n=== Texto Original ===\n\"\\x08\\x08\\x08\\x08A Turkish Bath sequence in a film noir located in New York in the 50's, that must be a hint at s\"\n\n=== Tokens Resultantes ===\n['a', 'turkish', 'bath', 'sequence', 'in', 'a', 'film', 'noir', 'located', 'in', 'new', 'york', 'in', 'the', 's']\n","output_type":"stream"}],"execution_count":12},{"id":"dd5a3156a01e191c","cell_type":"markdown","source":"#### 3. Eliminación de Stopwords\n\nEliminar las palabras vacías (stopwords) usando una lista estándar de la librería _nltk_.","metadata":{}},{"id":"9dd24aaa520624ec","cell_type":"code","source":"import nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n\nnltk.download('stopwords')\nnltk.download('punkt')\n\nstopwords_en = set(stopwords.words(\"english\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T22:25:18.787347Z","iopub.execute_input":"2025-11-17T22:25:18.787694Z","iopub.status.idle":"2025-11-17T22:25:21.038904Z","shell.execute_reply.started":"2025-11-17T22:25:18.787665Z","shell.execute_reply":"2025-11-17T22:25:21.037764Z"}},"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"}],"execution_count":13},{"id":"4be28ee6-82c6-447b-98bd-1fa56fd49730","cell_type":"code","source":"# Definicion de función que elimina los stopwords en inglés\ndef remove_stopwords(tokens):\n    return [t for t in tokens if t not in stopwords_en]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T22:25:24.159363Z","iopub.execute_input":"2025-11-17T22:25:24.159924Z","iopub.status.idle":"2025-11-17T22:25:24.165555Z","shell.execute_reply.started":"2025-11-17T22:25:24.159896Z","shell.execute_reply":"2025-11-17T22:25:24.164384Z"}},"outputs":[],"execution_count":14},{"id":"557188b0-0a55-4435-bb28-0206ce97dfac","cell_type":"code","source":"# Se crea la columna de tokens sin Stopwords\ndf['tokens_no_stop'] = df['tokens_norm'].apply(remove_stopwords)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T22:25:27.141909Z","iopub.execute_input":"2025-11-17T22:25:27.143214Z","iopub.status.idle":"2025-11-17T22:25:28.885228Z","shell.execute_reply.started":"2025-11-17T22:25:27.143167Z","shell.execute_reply":"2025-11-17T22:25:28.884047Z"}},"outputs":[],"execution_count":15},{"id":"86162194-c86d-437a-84e7-82ecc419356c","cell_type":"code","source":"# Se verifica la cantidad de tokens inicial y final, luego de aplicar la remoción de Stopwords\nidx = 14497\nprint(\"\\n=== Cantidad de tokens normalizados ===\")\nprint(len(df['tokens_norm'].iloc[idx]))\nprint(\"\\n=== Cantidad de tokens sin Stopwords ===\")\nprint(len(df['tokens_no_stop'].iloc[idx]))\nprint(\"\\nSe ha reducido casi a la mitad la cantidad de tokens\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T22:25:30.547800Z","iopub.execute_input":"2025-11-17T22:25:30.548186Z","iopub.status.idle":"2025-11-17T22:25:30.554442Z","shell.execute_reply.started":"2025-11-17T22:25:30.548160Z","shell.execute_reply":"2025-11-17T22:25:30.553548Z"}},"outputs":[{"name":"stdout","text":"\n=== Cantidad de tokens normalizados ===\n619\n\n=== Cantidad de tokens sin Stopwords ===\n313\n\nSe ha reducido casi a la mitad la cantidad de tokens\n","output_type":"stream"}],"execution_count":16},{"id":"755001c03938df5f","cell_type":"markdown","source":"#### 4. Stemming \n\nReducir el espacio de palabras","metadata":{}},{"id":"1477d8d1ffd23ea2","cell_type":"code","source":"from nltk.stem import PorterStemmer\nstemmer = PorterStemmer()","metadata":{"ExecuteTime":{"end_time":"2025-11-05T17:58:50.818939Z","start_time":"2025-11-05T17:58:50.444646Z"},"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T22:25:33.264245Z","iopub.execute_input":"2025-11-17T22:25:33.265138Z","iopub.status.idle":"2025-11-17T22:25:33.269512Z","shell.execute_reply.started":"2025-11-17T22:25:33.265104Z","shell.execute_reply":"2025-11-17T22:25:33.268433Z"}},"outputs":[],"execution_count":17},{"id":"f6dfa1ec-b7ae-429a-96b5-079eee495474","cell_type":"code","source":"# Definición de la función que aplica stemming a una lista de tokens\ndef apply_stemming(tokens):\n    \"\"\"Recibe lista de tokens (strings) -> devuelve lista de stems\"\"\"\n    if not isinstance(tokens, (list, tuple)):\n        return []\n    return [stemmer.stem(t) for t in tokens]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T22:25:35.401307Z","iopub.execute_input":"2025-11-17T22:25:35.401621Z","iopub.status.idle":"2025-11-17T22:25:35.407299Z","shell.execute_reply.started":"2025-11-17T22:25:35.401596Z","shell.execute_reply":"2025-11-17T22:25:35.406007Z"}},"outputs":[],"execution_count":18},{"id":"665d8208-167b-4f46-b34a-5a671c1793e0","cell_type":"code","source":"# Se aplica y se guarda en nueva columna del dataframe\ndf['tokens_stemmed'] = df['tokens_no_stop'].apply(apply_stemming)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T22:25:37.586475Z","iopub.execute_input":"2025-11-17T22:25:37.586970Z","iopub.status.idle":"2025-11-17T22:27:20.148464Z","shell.execute_reply.started":"2025-11-17T22:25:37.586911Z","shell.execute_reply":"2025-11-17T22:27:20.146743Z"}},"outputs":[],"execution_count":19},{"id":"9dd73e99-830f-46c7-b49b-28de548e3807","cell_type":"code","source":"# se visualiza una muestra de los tokens antes y después del stemming\nidx = 14497\nprint(\"\\n=== Muestra de Tokens sin Stopwords ===\")\nprint(df['tokens_no_stop'].iloc[idx][:30])\nprint(\"\\n=== Muestra de Tokens Stemmed ===\")\nprint(df['tokens_stemmed'].iloc[idx][:30])\nprint(\"\\nNo se reduce la cantidad de palabras, pero las que se recortan, se reducen hasta su raíz.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T22:31:01.550968Z","iopub.execute_input":"2025-11-17T22:31:01.552728Z","iopub.status.idle":"2025-11-17T22:31:01.564899Z","shell.execute_reply.started":"2025-11-17T22:31:01.552669Z","shell.execute_reply":"2025-11-17T22:31:01.562681Z"}},"outputs":[{"name":"stdout","text":"\n=== Muestra de Tokens sin Stopwords ===\n['turkish', 'bath', 'sequence', 'film', 'noir', 'located', 'new', 'york', 'must', 'hint', 'something', 'something', 'curiously', 'previous', 'comments', 'one', 'pointed', 'seems', 'essential', 'understanding', 'movie', 'turkish', 'baths', 'sequence', 'back', 'street', 'night', 'entrance', 'sleazy', 'sauna']\n\n=== Muestra de Tokens Stemmed ===\n['turkish', 'bath', 'sequenc', 'film', 'noir', 'locat', 'new', 'york', 'must', 'hint', 'someth', 'someth', 'curious', 'previou', 'comment', 'one', 'point', 'seem', 'essenti', 'understand', 'movi', 'turkish', 'bath', 'sequenc', 'back', 'street', 'night', 'entranc', 'sleazi', 'sauna']\n\nNo se reduce la cantidad de palabras, pero las que se recortan, se reducen hasta su raíz.\n","output_type":"stream"}],"execution_count":20},{"id":"3fe41d14a073d64","cell_type":"markdown","source":"#### 5. Verificar la diferencia\n\nComparar el tamaño del diccionario de términos del corpus antes y después de aplicar el preprocesamiento ","metadata":{}},{"id":"4cdfe98f-af8d-4b8b-a957-a10f15c71453","cell_type":"code","source":"def vocab_from_series_of_strings(series):\n    \"\"\"\n    Recibe una columna/iterable donde cada elemento es un string (texto).\n    Devuelve un set con todas las palabras únicas (separadas por espacios).\n    \"\"\"\n    vocab = set()                      # conjunto vacío donde acumulamos tipos únicos\n    for text in series:\n        if not isinstance(text, str):  # si la fila no es string (NaN, None, número) la saltamos\n            continue\n        words = text.split()           # separación simple por espacios\n        vocab.update(words)            # añade todas las palabras (ignora duplicados)\n    return vocab\n\n\ndef vocab_from_series_of_tokenlists(series):\n    \"\"\"\n    Recibe una columna/iterable donde cada elemento es una lista (o tupla) de tokens.\n    Devuelve un set con todos los tokens únicos.\n    Si alguna fila es string en lugar de lista, la tokeniza por espacios para manejarlo.\n    \"\"\"\n    vocab = set()\n    for item in series:\n        # si ya es lista/tupla se itera directamente\n        if isinstance(item, (list, tuple)):\n            vocab.update(item)\n        # si es string, lo tokenizamos por espacios (soporte robusto)\n        elif isinstance(item, str):\n            vocab.update(item.split())\n        # si es otro tipo (NaN, None, número) se ignora\n    return vocab\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T22:46:52.832886Z","iopub.execute_input":"2025-11-17T22:46:52.834274Z","iopub.status.idle":"2025-11-17T22:46:52.922752Z","shell.execute_reply.started":"2025-11-17T22:46:52.834234Z","shell.execute_reply":"2025-11-17T22:46:52.921617Z"}},"outputs":[],"execution_count":26},{"id":"9283a072-c20b-4e21-bbc5-969c1f39366a","cell_type":"code","source":"# 1) Raw vocab (word forms tal cual del original, tokenizados por espacios)\nraw_vocab = vocab_from_series_of_strings(df['review_original'])\n\n# 2) Clean vocab (texto limpio, antes de normalizar a minúsculas o quitar números)\nclean_vocab = vocab_from_series_of_strings(df['review_clean'])\n\n# 3) Normalized vocab (tokens_norm es lista de tokens ya normalizados y sin números)\nnorm_vocab = vocab_from_series_of_tokenlists(df['tokens_norm'])\n\n# 4) No-stop vocab\nno_stop_vocab = vocab_from_series_of_tokenlists(df['tokens_no_stop'])\n\n# 5) Stemmed vocab\nstemmed_vocab = vocab_from_series_of_tokenlists(df['tokens_stemmed'])\n\n# Mostrar resultados\netapasVocab = {\n    'raw': raw_vocab,\n    'clean': clean_vocab,\n    'normalized': norm_vocab,\n    'no_stopwords': no_stop_vocab,\n    'stemmed': stemmed_vocab\n}\n\nfor name, vocab in etapasVocab.items():\n    print(f\"vocab. {name:13s}: {len(vocab):7d} palabras\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T22:57:57.231098Z","iopub.execute_input":"2025-11-17T22:57:57.231413Z","iopub.status.idle":"2025-11-17T22:58:02.398888Z","shell.execute_reply.started":"2025-11-17T22:57:57.231389Z","shell.execute_reply":"2025-11-17T22:58:02.397979Z"}},"outputs":[{"name":"stdout","text":"vocab. raw          :  438729 palabras\nvocab. clean        :  129426 palabras\nvocab. normalized   :   99319 palabras\nvocab. no_stopwords :   99166 palabras\nvocab. stemmed      :   68921 palabras\n","output_type":"stream"}],"execution_count":41},{"id":"e24e1809-f606-4036-ade1-e8ba570184a8","cell_type":"code","source":"def reduction(a, b):\n    diferencia = len(a) - len(b)\n    if len(a) > 0:\n        porcentaje = diferencia * 100.0/len(a)\n    else:\n        porcentaje = 0.0\n        \n    porcentaje_str = f\"{porcentaje:.2f} %\"\n    return diferencia, porcentaje_str","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T22:51:14.251826Z","iopub.execute_input":"2025-11-17T22:51:14.252976Z","iopub.status.idle":"2025-11-17T22:51:14.259006Z","shell.execute_reply.started":"2025-11-17T22:51:14.252909Z","shell.execute_reply":"2025-11-17T22:51:14.257758Z"}},"outputs":[],"execution_count":30},{"id":"ab719703-41ea-4467-8440-f8d56b5cf7af","cell_type":"code","source":"# Comparación de los tamaños de diccionario antes y después del preprocesamiento en cada etapa\n# se representa (cantPalabras, porcentajeDeReducción)\nprint(\"raw -> clean:\", reduction(raw_vocab, clean_vocab))\nprint(\"clean -> normalized:\", reduction(clean_vocab, norm_vocab))\nprint(\"normalized -> no_stopwords:\", reduction(norm_vocab, no_stop_vocab))\nprint(\"no_stopwords -> stemmed:\", reduction(no_stop_vocab, stemmed_vocab))\nprint(\"raw -> stemmed:\", reduction(raw_vocab, stemmed_vocab))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T22:31:59.119508Z","iopub.execute_input":"2025-11-17T22:31:59.119807Z","iopub.status.idle":"2025-11-17T22:31:59.127206Z","shell.execute_reply.started":"2025-11-17T22:31:59.119785Z","shell.execute_reply":"2025-11-17T22:31:59.125807Z"}},"outputs":[{"name":"stdout","text":"raw -> clean: (309303, '70.50 %')\nclean -> normalized: (30107, '23.26 %')\nnormalized -> no_stopwords: (153, '0.15 %')\nno_stopwords -> stemmed: (30245, '30.50 %')\nraw -> stemmed: (369808, '84.29 %')\n","output_type":"stream"}],"execution_count":25}]}